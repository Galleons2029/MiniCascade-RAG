# -*- coding: utf-8 -*-
"""
Intent Agent æ€§èƒ½æµ‹è¯•

æµ‹è¯• Intent Agent çš„å“åº”æ—¶é—´ã€å‡†ç¡®ç‡å’Œèµ„æºä½¿ç”¨æƒ…å†µ
"""

import time
import asyncio
import statistics
from typing import List, Dict
from datetime import datetime

from app.core.agent.graph.intent_agent import build_unified_agent_graph
from langchain_openai import ChatOpenAI


class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    def __init__(self, llm):
        self.llm = llm
        self.graph = build_unified_agent_graph(llm)
        self.results = []

    async def test_response_time(self, test_cases: List[Dict], iterations: int = 5):
        """æµ‹è¯•å“åº”æ—¶é—´"""
        print(f"ğŸš€ å¼€å§‹å“åº”æ—¶é—´æµ‹è¯• ({iterations} æ¬¡è¿­ä»£)")

        for test_case in test_cases:
            case_times = []

            for i in range(iterations):
                start_time = time.time()

                try:
                    await self.graph.ainvoke(
                        {"messages": [{"role": "user", "content": test_case["message"]}], "session_id": f"perf-test-{i}"}
                    )

                    end_time = time.time()
                    response_time = end_time - start_time
                    case_times.append(response_time)

                except Exception as e:
                    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
                    continue

            if case_times:
                avg_time = statistics.mean(case_times)
                min_time = min(case_times)
                max_time = max(case_times)

                self.results.append(
                    {
                        "test_case": test_case["name"],
                        "message": test_case["message"],
                        "avg_response_time": avg_time,
                        "min_response_time": min_time,
                        "max_response_time": max_time,
                        "iterations": len(case_times),
                    }
                )

                print(f"ğŸ“Š {test_case['name']}: å¹³å‡ {avg_time:.2f}s (æœ€å°: {min_time:.2f}s, æœ€å¤§: {max_time:.2f}s)")

    async def test_accuracy(self, test_cases: List[Dict]):
        """æµ‹è¯•å‡†ç¡®ç‡"""
        print("ğŸ¯ å¼€å§‹å‡†ç¡®ç‡æµ‹è¯•")

        correct_predictions = 0
        total_predictions = 0

        for test_case in test_cases:
            try:
                result = await self.graph.ainvoke(
                    {
                        "messages": [{"role": "user", "content": test_case["message"]}],
                        "session_id": f"accuracy-test-{total_predictions}",
                    }
                )

                predicted_intent = result.get("intent", "unknown")
                expected_intent = test_case["expected_intent"]

                if predicted_intent == expected_intent:
                    correct_predictions += 1
                    status = "âœ…"
                else:
                    status = "âŒ"

                total_predictions += 1

                print(f"{status} {test_case['name']}: é¢„æœŸ {expected_intent}, å®é™… {predicted_intent}")

            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥ {test_case['name']}: {e}")
                total_predictions += 1

        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        print(f"ğŸ“ˆ æ€»ä½“å‡†ç¡®ç‡: {accuracy:.2%} ({correct_predictions}/{total_predictions})")

        return accuracy

    async def test_concurrent_requests(self, message: str, concurrent_count: int = 10):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚å¤„ç†èƒ½åŠ›"""
        print(f"âš¡ å¼€å§‹å¹¶å‘æµ‹è¯• ({concurrent_count} ä¸ªå¹¶å‘è¯·æ±‚)")

        async def single_request(request_id: int):
            start_time = time.time()
            try:
                result = await self.graph.ainvoke(
                    {"messages": [{"role": "user", "content": message}], "session_id": f"concurrent-test-{request_id}"}
                )
                end_time = time.time()
                return {
                    "request_id": request_id,
                    "success": True,
                    "response_time": end_time - start_time,
                    "intent": result.get("intent", "unknown"),
                }
            except Exception as e:
                end_time = time.time()
                return {
                    "request_id": request_id,
                    "success": False,
                    "response_time": end_time - start_time,
                    "error": str(e),
                }

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [single_request(i) for i in range(concurrent_count)]

        # æ‰§è¡Œå¹¶å‘è¯·æ±‚
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        total_time = time.time() - start_time

        # åˆ†æç»“æœ
        successful_requests = [r for r in results if r["success"]]
        failed_requests = [r for r in results if not r["success"]]

        if successful_requests:
            avg_response_time = statistics.mean([r["response_time"] for r in successful_requests])
            throughput = len(successful_requests) / total_time
        else:
            avg_response_time = 0
            throughput = 0

        print("ğŸ“Š å¹¶å‘æµ‹è¯•ç»“æœ:")
        print(f"   æ€»è¯·æ±‚æ•°: {concurrent_count}")
        print(f"   æˆåŠŸè¯·æ±‚: {len(successful_requests)}")
        print(f"   å¤±è´¥è¯·æ±‚: {len(failed_requests)}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s")
        print(f"   ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}s")

        return {
            "total_requests": concurrent_count,
            "successful_requests": len(successful_requests),
            "failed_requests": len(failed_requests),
            "avg_response_time": avg_response_time,
            "throughput": throughput,
            "total_time": total_time,
        }

    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š Intent Agent æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        if self.results:
            print("\nâ±ï¸  å“åº”æ—¶é—´ç»Ÿè®¡:")
            for result in self.results:
                print(f"   {result['test_case']}: {result['avg_response_time']:.2f}s")

            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            all_avg_times = [r["avg_response_time"] for r in self.results]
            overall_avg = statistics.mean(all_avg_times)
            print(f"\nğŸ“ˆ æ€»ä½“å¹³å‡å“åº”æ—¶é—´: {overall_avg:.2f}s")

        print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        if self.results:
            slowest = max(self.results, key=lambda x: x["avg_response_time"])
            if slowest["avg_response_time"] > 2.0:
                print("   - è€ƒè™‘ä¼˜åŒ– LLM è°ƒç”¨æ¬¡æ•°")
                print("   - æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ")
                print("   - è€ƒè™‘ä½¿ç”¨ç¼“å­˜æœºåˆ¶")

        print("   - ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ")
        print("   - è€ƒè™‘å¼‚æ­¥å¤„ç†ä¼˜åŒ–")


async def run_performance_tests():
    """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    # åˆå§‹åŒ– LLM
    import os

    api_key = os.getenv("LLM_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ éœ€è¦è®¾ç½® LLM_API_KEY æˆ– OPENAI_API_KEY")
        return

    llm = ChatOpenAI(
        model=os.getenv("LLM_MODEL", "gpt-3.5-turbo"),
        temperature=0.1,
        api_key=api_key,
        base_url=os.getenv("LLM_BASE_URL"),
    )

    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = PerformanceTestSuite(llm)

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {"name": "ç®€å•QA", "message": "ä»€ä¹ˆæ˜¯AIï¼Ÿ", "expected_intent": "qa"},
        {"name": "å¤æ‚QA", "message": "ä¸Šå‘¨çš„é”€å”®æ•°æ®æ˜¾ç¤ºäº†ä»€ä¹ˆè¶‹åŠ¿ï¼Ÿ", "expected_intent": "qa"},
        {"name": "å†™ä½œä»»åŠ¡", "message": "è¯·å¸®æˆ‘å†™ä¸€ä»½æŠ¥å‘Š", "expected_intent": "write"},
        {"name": "æœç´¢ä»»åŠ¡", "message": "æœç´¢æœ€æ–°è®ºæ–‡", "expected_intent": "search"},
        {"name": "æ‰§è¡Œä»»åŠ¡", "message": "æ‰§è¡Œå¤‡ä»½", "expected_intent": "exec"},
        {"name": "é—²èŠ", "message": "ä½ å¥½", "expected_intent": "smalltalk"},
    ]

    # è¿è¡Œæµ‹è¯•
    await test_suite.test_response_time(test_cases, iterations=3)
    accuracy = await test_suite.test_accuracy(test_cases)
    concurrent_result = await test_suite.test_concurrent_requests("ä»€ä¹ˆæ˜¯RAGç³»ç»Ÿï¼Ÿ", concurrent_count=5)

    # ç”ŸæˆæŠ¥å‘Š
    test_suite.generate_performance_report()

    return {"response_time_results": test_suite.results, "accuracy": accuracy, "concurrent_result": concurrent_result}


if __name__ == "__main__":
    """ç›´æ¥è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    asyncio.run(run_performance_tests())
